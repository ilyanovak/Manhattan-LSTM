{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ManhattanLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzE/WtfC3gxyPBOND4koRm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilyanovak/Manhattan-LSTM/blob/main/ManhattanLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg4SjjYq-xBY"
      },
      "source": [
        "\n",
        "# TODO\n",
        "1.   Change location of importint data\n",
        "2.   Methods comments\n",
        "3.   Data requirements check\n",
        "4.   assertions for sanity checks, max_seq_length\n",
        "5.   implement sample_size\n",
        "6.   Custom vocabulary method\n",
        "7.   Add comments to each description\n",
        "8.   Fix save_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhaW2gqSVnjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51fd891c-78e9-4ace-89e4-5b3133777e88"
      },
      "source": [
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import random\n",
        "import re\n",
        "import spacy\n",
        "import spacy.cli\n",
        "import time\n",
        "from numpy import loadtxt\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "spacy.cli.download(\"en_core_web_lg\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5rL1Iem6lsS"
      },
      "source": [
        "quora_source = 'https://raw.githubusercontent.com/ilyanovak/Manhattan-LSTM/main/data/quora.csv'\n",
        "sample_training_data = pd.read_csv(quora_source, engine='python',\n",
        "                          usecols=['question1', 'question2', 'is_duplicate'],\n",
        "                        skiprows=random.sample(range(1, 404290), 404290-100))\n",
        "lstm = ManhatanLSTM(sample_training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUbP5iLawVNn",
        "outputId": "89f5314d-1ccb-436b-9ead-d52c022aa39a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lstm.evaluate_model()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 1.0004427433013916\n",
            "binary_accuracy: 65.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pivy4aorOnyI"
      },
      "source": [
        "class ManhatanLSTM:\n",
        "\n",
        "    def __init__ (self, training_data):\n",
        "\n",
        "        # Verify training data\n",
        "\n",
        "        self._training_data = training_data\n",
        "\n",
        "\n",
        "    def __text_to_word_list(self, text):\n",
        "        '''\n",
        "        Helper method used to preprocess text. \n",
        "        text: A sequence of words of string type\n",
        "        Returns text with all characters removed except for letters of the \n",
        "        alphabet and space character. All letters are converted to lower case\n",
        "        '''\n",
        "\n",
        "        text = str(text).lower()\n",
        "        text = re.sub('[^a-z ]', '', text)\n",
        "        text = text\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def __create_vocabulary(self):\n",
        "        '''\n",
        "        \n",
        "        '''\n",
        "\n",
        "        self._vocabulary = {}\n",
        "\n",
        "        # '<unk>' will never be used, its only a placeholder for the [0, 0, ....0] embedding\n",
        "        inverse_vocabulary = ['<unk>']  \n",
        "\n",
        "        # Iterate through each sequence\n",
        "        for idx, sequence in enumerate([self._training_data.columns[0], self._training_data.columns[1]]):    \n",
        "\n",
        "            # Duplicate sequence columns that will be transformed into vectors\n",
        "            self._training_data[f'sequence{idx+1}_vect'] = self._training_data[sequence]\n",
        "\n",
        "            # Iterate through each row\n",
        "            for index in tqdm(range(len(self._training_data))):\n",
        "\n",
        "                # Preprocess sequence text\n",
        "                text_old = self._training_data.loc[index, sequence]\n",
        "                text_new = self.__text_to_word_list(text_old)\n",
        "                self._training_data.at[index, sequence] = text_new\n",
        "\n",
        "                word2id = []\n",
        "\n",
        "                # Iterate through each word in the sequence\n",
        "                for word in text_new.split(\" \"):\n",
        "\n",
        "                    # Create id for word if its not in vocabulary yet\n",
        "                    # Add word to text vector\n",
        "                    if word not in self._vocabulary:\n",
        "                        self._vocabulary[word] = len(inverse_vocabulary)\n",
        "                        word2id.append(self._vocabulary[word])\n",
        "                        inverse_vocabulary.append(word)\n",
        "                    else:\n",
        "                        word2id.append(self._vocabulary[word])\n",
        "\n",
        "                self._training_data.at[index, f'sequence{idx+1}_vect'] = word2id\n",
        "\n",
        "\n",
        "    def __create_embeddings(self):\n",
        "\n",
        "        nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "        # Create embeddings matrix based on vocabulary\n",
        "        embedding_dim=300\n",
        "        self._embeddings = np.zeros((len(self._vocabulary.items()) + 1, embedding_dim))\n",
        "\n",
        "        for word, index in tqdm(self._vocabulary.items()):\n",
        "            self._embeddings[index] = nlp(word).vector\n",
        "\n",
        "\n",
        "    def __create_train_validation(self):\n",
        "\n",
        "        # Create training and validation data\n",
        "        train, val = train_test_split(self._training_data, test_size=0.2)\n",
        "\n",
        "        # Create dictionaries with left/right keys for two sets of sequences \n",
        "        self._X_train = {'left':train['sequence1_vect'], 'right':train['sequence2_vect']}\n",
        "        self._Y_train = train['is_duplicate'].values\n",
        "\n",
        "        self._X_val = {'left':val['sequence1_vect'], 'right':val['sequence2_vect']}\n",
        "        self._Y_val = val['is_duplicate'].values\n",
        "\n",
        "        # Calculate maximum length of sequences\n",
        "        self._max_seq_length = max(train['sequence1_vect'].apply(lambda x: len(x)).max(),\n",
        "                                  train['sequence2_vect'].apply(lambda x: len(x)).max(),\n",
        "                                  val['sequence1_vect'].apply(lambda x: len(x)).max(),\n",
        "                                  val['sequence2_vect'].apply(lambda x: len(x)).max())\n",
        "\n",
        "        # Zero padding\n",
        "        for self._training_data in [self._X_train, self._X_val]:\n",
        "            for side in ['left', 'right']:\n",
        "                self._training_data[side] = pad_sequences(self._training_data[side], maxlen=self._max_seq_length, padding='post')\n",
        "\n",
        "        # Sanity check\n",
        "        assert(self._X_train['left'].shape == self._X_train['right'].shape)\n",
        "        assert(self._X_val['left'].shape == self._X_val['right'].shape)\n",
        "\n",
        "\n",
        "    def __exponent_neg_manhattan_distance(self, left, right):\n",
        "\n",
        "        ''' Calculates manhattan similarity estimate of the LSTMs outputs'''\n",
        "        return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "\n",
        "    def __create_model(self, ):\n",
        "\n",
        "        # Left and right input layers\n",
        "        left_input, right_input = Input(shape=(self._max_seq_length,)), Input(shape=(self._max_seq_length,))\n",
        "\n",
        "        # Embeddings layer\n",
        "        embedding_dim = 300\n",
        "        embedding_layer = Embedding(input_dim=len(self._embeddings), # Size of vocabulary\n",
        "                                    output_dim=embedding_dim, # Dimension of dense embedding\n",
        "                                    weights=[self._embeddings], # ???\n",
        "                                    trainable=False, #???\n",
        "                                    input_length=self._max_seq_length # Length of input sequences\n",
        "                                    )\n",
        "\n",
        "        left_embedding, right_embedding = embedding_layer(left_input), embedding_layer(right_input)\n",
        "\n",
        "        # LSTM later that is used on both sides\n",
        "        lstm = LSTM(64)\n",
        "\n",
        "        left_output, right_output = lstm(left_embedding), lstm(right_embedding)\n",
        "\n",
        "        # Calculate manhattan distance for LSTM\n",
        "        manhat_dist = Lambda(function=lambda x: self.__exponent_neg_manhattan_distance(x[0], x[1]),\n",
        "                            output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "        # Setup model\n",
        "        self.model = Model(inputs=[left_input, right_input], outputs=[manhat_dist])\n",
        "\n",
        "\n",
        "    def __compile_model(self):\n",
        "\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer='nadam', metrics='binary_accuracy')\n",
        "\n",
        "\n",
        "    def __fit_model(self, batch_size=32, epochs=15):\n",
        "\n",
        "        time_start = time.time()\n",
        "\n",
        "        self.model_history = self.model.fit([self._X_train['left'], self._X_train['right']], self._Y_train, \n",
        "                            batch_size=batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=([self._X_val['left'], self._X_val['right']], self._Y_val))\n",
        "\n",
        "        time_end = time.time()\n",
        "\n",
        "        print(f'Training time is approximately {int((time_end - time_start) / 60)} minutes')\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        print('Creating Vocabulary...')\n",
        "        print('--------------------\\n')\n",
        "\n",
        "        self.__create_vocabulary()\n",
        "        \n",
        "        print('\\n\\nCreating embeddings...')\n",
        "        print('--------------------\\n')\n",
        "        self.__create_embeddings()\n",
        "        \n",
        "\n",
        "        print('\\n\\nCreating training and validation datasets...')\n",
        "        print('--------------------\\n')\n",
        "        self.__create_train_validation()\n",
        "        \n",
        "\n",
        "        print('\\n\\nCreating model...')\n",
        "        print('--------------------\\n')\n",
        "        self.__create_model()\n",
        "        \n",
        "\n",
        "        print('\\n\\nCompiling model...')\n",
        "        print('--------------------\\n')\n",
        "        self.__compile_model()\n",
        "        \n",
        "\n",
        "        print('\\n\\nFitting model...')\n",
        "        print('-------------------\\n')\n",
        "        self.__fit_model()\n",
        "        \n",
        "\n",
        "\n",
        "    def summarize_model(self):\n",
        "        # summarize model.\n",
        "        self.model.summary()\n",
        "\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        # Evaluate the model\n",
        "        score = self.model.evaluate([self._X_val['left'], self._X_val['right']], self._Y_val, verbose=0)\n",
        "        print(self.model.metrics_names[0], score[0])\n",
        "        print(\"%s: %.2f%%\" % (self.model.metrics_names[1], score[1]*100))\n",
        "\n",
        "\n",
        "    def plot_model_accuracy_and_loss(self):\n",
        "\n",
        "        fig = go.Figure(data=go.Scatter(x=list(range(1,len(self.model_history.history['binary_accuracy'])+1)), \n",
        "                                        y=self.model_history.history['binary_accuracy'],\n",
        "                                        name='Train'))\n",
        "        fig.add_trace(go.Scatter(x=list(range(1,len(self.model_history.history['val_binary_accuracy'])+1)), \n",
        "                                y=self.model_history.history['val_binary_accuracy'],\n",
        "                                name='Validation'))\n",
        "        fig.update_layout(title_text='Model Accuracy',\n",
        "                        title_x=0.5,\n",
        "                        xaxis_title_text='Epoch',\n",
        "                        yaxis_title_text='Accuracy',\n",
        "                        height=400,\n",
        "                        width=800,\n",
        "                        plot_bgcolor ='#FFFFFF',\n",
        "                        xaxis_linecolor='#000000',\n",
        "                        xaxis_linewidth=2,\n",
        "                        xaxis_mirror=True,\n",
        "                        yaxis_linecolor='#000000',\n",
        "                        yaxis_linewidth=2,\n",
        "                        yaxis_mirror=True)\n",
        "\n",
        "        fig2 = go.Figure(data=go.Scatter(x=list(range(1,len(self.model_history.history['loss'])+1)), \n",
        "                                        y=self.model_history.history['loss'],\n",
        "                                        name='Train'))\n",
        "        fig2.add_trace(go.Scatter(x=list(range(1,len(self.model_history.history['val_loss'])+1)), \n",
        "                                y=self.model_history.history['val_loss'],\n",
        "                                name='Validation'))\n",
        "        fig2.update_layout(title_text='Model Loss',\n",
        "                        title_x=0.5,\n",
        "                        xaxis_title_text='Epoch',\n",
        "                        yaxis_title_text='Loss',\n",
        "                        height=400,\n",
        "                        width=800,\n",
        "                        plot_bgcolor ='#FFFFFF',\n",
        "                        xaxis_linecolor='#000000',\n",
        "                        xaxis_mirror=True,\n",
        "                        xaxis_linewidth=2,\n",
        "                        yaxis_linecolor='#000000',\n",
        "                        yaxis_mirror=True,\n",
        "                        yaxis_linewidth=2,\n",
        "        )\n",
        "\n",
        "        fig.show()\n",
        "        fig2.show()\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        # save model and architecture to single file\n",
        "        self.model.save(\"model.h5\")\n",
        "        print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "    def load_model(self, path):\n",
        "        # load model\n",
        "        self.model = load_model(path)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu9wjcoXw3vO"
      },
      "source": [
        "quora_source = 'https://raw.githubusercontent.com/ilyanovak/Manhattan-LSTM/main/data/quora.csv'\n",
        "sample_training_data = pd.read_csv(quora_source, engine='python',\n",
        "                                   usecols=['question1', 'question2', 'is_duplicate'],\n",
        "                                   skiprows=random.sample(range(1, 404290), 404290-100))\n",
        "lstm = ManhatanLSTM(sample_training_data)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv6llgd27sI-",
        "outputId": "93e32f98-0b03-4796-b5fd-df69e5d8ebf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lstm.fit()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 11714.95it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 10178.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating Vocabulary...\n",
            "--------------------\n",
            "\n",
            "\n",
            "\n",
            "Creating embeddings...\n",
            "--------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 727/727 [00:06<00:00, 109.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Creating training and validation datasets...\n",
            "--------------------\n",
            "\n",
            "max_seq_length: 39\n",
            "\n",
            "\n",
            "Creating model...\n",
            "--------------------\n",
            "\n",
            "\n",
            "\n",
            "Compiling model...\n",
            "--------------------\n",
            "\n",
            "\n",
            "\n",
            "Fitting model...\n",
            "-------------------\n",
            "\n",
            "Epoch 1/15\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 1.4971 - binary_accuracy: 0.4875 - val_loss: 1.6743 - val_binary_accuracy: 0.2500\n",
            "Epoch 2/15\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.0370 - binary_accuracy: 0.5125 - val_loss: 1.2946 - val_binary_accuracy: 0.3000\n",
            "Epoch 3/15\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.7735 - binary_accuracy: 0.6125 - val_loss: 1.0397 - val_binary_accuracy: 0.4000\n",
            "Epoch 4/15\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.5993 - binary_accuracy: 0.6500 - val_loss: 0.7852 - val_binary_accuracy: 0.5500\n",
            "Epoch 5/15\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.4916 - binary_accuracy: 0.7875 - val_loss: 0.6305 - val_binary_accuracy: 0.7000\n",
            "Epoch 6/15\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.3883 - binary_accuracy: 0.8750 - val_loss: 0.5884 - val_binary_accuracy: 0.6500\n",
            "Epoch 7/15\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.2914 - binary_accuracy: 0.8750 - val_loss: 0.7358 - val_binary_accuracy: 0.5500\n",
            "Epoch 8/15\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.2631 - binary_accuracy: 0.9375 - val_loss: 0.6062 - val_binary_accuracy: 0.7000\n",
            "Epoch 9/15\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.2362 - binary_accuracy: 0.9250 - val_loss: 0.4660 - val_binary_accuracy: 0.6500\n",
            "Epoch 10/15\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.2284 - binary_accuracy: 0.9375 - val_loss: 0.5649 - val_binary_accuracy: 0.6500\n",
            "Epoch 11/15\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1677 - binary_accuracy: 0.9625 - val_loss: 0.4582 - val_binary_accuracy: 0.7000\n",
            "Epoch 12/15\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1886 - binary_accuracy: 0.9375 - val_loss: 0.6040 - val_binary_accuracy: 0.7000\n",
            "Epoch 13/15\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.1926 - binary_accuracy: 0.9500 - val_loss: 0.7128 - val_binary_accuracy: 0.6500\n",
            "Epoch 14/15\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.1589 - binary_accuracy: 0.9625 - val_loss: 0.5128 - val_binary_accuracy: 0.7500\n",
            "Epoch 15/15\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.1352 - binary_accuracy: 0.9875 - val_loss: 0.5964 - val_binary_accuracy: 0.6500\n",
            "Training time is approximately 0 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gAQ-fr7xUtJ",
        "outputId": "4d75c8b8-85d6-47f7-f8bc-bd3177f34816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "lstm.plot_model_accuracy_and_loss()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-b7fa0fa48eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'ManhatanLSTM' object has no attribute 'model_history'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKTnzr2XAcb4"
      },
      "source": [
        "# TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbpu0DvuAdfm"
      },
      "source": [
        "# ################################\n",
        "# # Import leafly strains and concatinate each strain's feelings, helps, negatives and description into a single column\n",
        "\n",
        "# strains = pd.read_json('/content/leafly.json')\n",
        "# strains['strain_text'] = pd.Series(dtype='str')\n",
        "# strains = strains.replace({None:\"\"})\n",
        "\n",
        "# columns = ['feeling_1', 'feeling_2', 'feeling_3', 'feeling_4', 'feeling_5',\n",
        "#            'helps_1', 'helps_2', 'helps_3', 'helps_4', 'helps_5',\n",
        "#            'negative_1', 'negative_2', 'negative_3', 'negative_4', 'negative_5',\n",
        "#            'description']\n",
        "\n",
        "# for i in range(0, len(strains)):\n",
        "#     concat = \"\"\n",
        "#     for col in columns:\n",
        "#         concat = concat + \" \" + strains.loc[i, col]\n",
        "#     strains.loc[i, 'strain_text'] = concat\n",
        "\n",
        "# ################################\n",
        "\n",
        "# '''\n",
        "# This is an example of fake text submit by user in the website's UI.\n",
        "# It is a concatination of user's selected feelings, helps and negatives terms.\n",
        "# It also concatinates the user's submitted description that is written in free form.\n",
        "# It is used to test the neural network model's predictive accuracy on marijuana strains.\n",
        "# The text can be changed to test different scenarios.\n",
        "# '''\n",
        "# user_text = 'Headache Dizzy Nausea Happy Creative I am very sad every day feeling terrible and I need something that will cure my insomnia and anxiety and will help me get out of bed each day.'\n",
        "\n",
        "# # Add user text to strains data\n",
        "# print('user_text:', user_text)\n",
        "# ################################\n",
        "\n",
        "# strains[['strain', 'strain_text']].head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}